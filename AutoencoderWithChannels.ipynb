{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e90170",
   "metadata": {},
   "source": [
    "Objetive of the project could be finding a regularisation of the latent space to extract high-level features of the signals obtained from the Sauron Semiconductors placed inside the AGATA.\n",
    "\n",
    "Things to do:\n",
    "-Solve errors of Classifier.py.\n",
    "-Find regularisation strategies from the book ML for Physicist in this unsupervised autoencoder.\n",
    "-Add .root dataset of Daniele Mengoni to the code.\n",
    "-Improve the noise reduction by including other techniques to the dataset (e.g. cross-validation).\n",
    "-Add axis units.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61f5878a-f63d-4176-95bc-635d4dd1f85d",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/luna/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/luna/.cache/torch_extensions/py311_cu118/PIDDataset/build.ninja...\n",
      "Building extension module PIDDataset...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module PIDDataset...\n"
     ]
    }
   ],
   "source": [
    "#Loading the libraries and compiling the c++ module\n",
    "\n",
    "from torch.utils.cpp_extension import load\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib\n",
    "import pytorch_model_summary as pms\n",
    "\n",
    "from autoencoder import *\n",
    "\n",
    "#Compile the c++ module\n",
    "pid_dataset = load(name=\"PIDDataset\",\n",
    "                   sources=[\"PidDataset.cpp\",\"ReadRawData.cpp\",\"ReadBinaryData.cpp\", \"mwdlib.cpp\"],\n",
    "                   verbose=True,\n",
    "                   extra_cflags=['-O3'],\n",
    "                  )\n",
    "\n",
    "# TO SHOW INTERACTIVE PLOT\n",
    "%matplotlib widget\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dacbaf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define NN parameters\n",
    "\n",
    "\n",
    "derivative_order=1\n",
    "normalized=True\n",
    "convolutional=True #I can try to put this to False!!!\n",
    "variational=False #Variational autoencoder.\n",
    "dropout_p=0.0 #In big NN we can put this parameter to 50% to avoid overfitting, by eliminating neurons. In this case, a smaller NN, it is not that useful.\n",
    "latent_dim_list = [2]\n",
    "nLayers_list = [4]\n",
    "#act_fn = torch.nn.ReLU\n",
    "act_fn = torch.nn.GELU #Activation function\n",
    "\n",
    "#Training parameters\n",
    "batch_size = 1000 #this was working. \n",
    "learning_rate = 5e-3 #If it's too big, it may go out of the gradient descent.\n",
    "#batch_size = 5000 #this only makes things slow\n",
    "retrain=False #??\n",
    "version_nr = 0\n",
    "EarlyStoppingNr = 15\n",
    "max_epochs = 300\n",
    "exclude_outliers = False\n",
    "device = \"cpu\"\n",
    "\n",
    "#Dataset\n",
    "sauronData = False\n",
    "\n",
    "if sauronData:\n",
    "    trainDatasetFile = \"./DataFusEvSauron/RU_caendig_i1468_0005_0000.caendat\"\n",
    "    datasetEvents = 3_000_000 #Events=Number of total signals.\n",
    "    samples_number=128\n",
    "    first_sample=0\n",
    "    n_channels=64\n",
    "else:\n",
    "    trainDatasetFile = \"./DataLiFOscar/data_0-7.caendat\"\n",
    "    datasetEvents = 709_755\n",
    "    samples_number=40\n",
    "    first_sample=200\n",
    "    n_channels=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f527dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "     ConvEncoder-1              [3, 2]           2,546           2,546\n",
      "     ConvDecoder-2       [3, 1, 1, 40]           2,317           2,317\n",
      "=======================================================================\n",
      "Total params: 4,863\n",
      "Trainable params: 4,863\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Debug cell???\n",
    "\n",
    "for nLayers in nLayers_list:\n",
    "   for latent_dim in latent_dim_list:\n",
    "       autoenctest = Autoencoder(base_channel_size=nLayers, \n",
    "                                    latent_dim=latent_dim, \n",
    "                                    encoder_class=ConvEncoder, \n",
    "                                    decoder_class=ConvDecoder, \n",
    "                                    width=derivative_order, \n",
    "                                    height=samples_number, \n",
    "                                    act_fn=act_fn, \n",
    "                                    dropout_p=dropout_p, \n",
    "                                    learning_rate=learning_rate)\n",
    "   \n",
    "       x1_s = torch.zeros([3, 1, derivative_order, samples_number], dtype=torch.float32)\n",
    "       x1_c = torch.zeros([3, 64+1], dtype=torch.float32)\n",
    "\n",
    "       x2_s = torch.zeros([3, latent_dim], dtype=torch.float32)\n",
    "\n",
    "       print(pms.summary(autoenctest, x1_s, x1_c))\n",
    "       #print(autoenctest._get_reconstruction_loss([x1_s, x1_c]))\n",
    "       #print(autoenctest.forward(x1_s, x1_c).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f005f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 709755 events\n",
      "Number of board in the stream 5\n",
      "Epoch start = 0x657C613D\n"
     ]
    }
   ],
   "source": [
    "#Load the dataset\n",
    "\n",
    "split_percent = [0.9, 0.05, 0.05] #90% for training, 5% for testing and 5% for validating.\n",
    "#This was a normal splitting but we could add cross-validation after???\n",
    "split = [int(i*datasetEvents) for i in split_percent] #Multiply the number of total events to each percentage. Output would be, if datasetEvents=1000, [900, 5, 5].\n",
    "\n",
    "split[0] += datasetEvents - sum(split) #To ensure the total always matches datasetEvents.\n",
    "\n",
    "[train_dataset, val_dataset, test_dataset] = torch.utils.data.random_split(pid_dataset.PIDDataset(root=trainDatasetFile, \n",
    "                                       mode=pid_dataset.kTrain, \n",
    "                                       count=datasetEvents, \n",
    "                                       nder=derivative_order, \n",
    "                                       nsamples=samples_number, \n",
    "                                       firstsample=first_sample,\n",
    "                                       nchannels=n_channels,\n",
    "                                       normalized=normalized),\n",
    "                                       split)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "#Batch size: number of signals per batch.\n",
    "#Shuffle: shuffle data at every epoch.\n",
    "#Epoch: During an epoch, the model processes each training example once, \n",
    "#and the learning process updates the model's parameters based on the loss calculated from the training examples.\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=2000, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2000, shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "def get_train_images(num):\n",
    "    return [torch.stack([train_dataset[i][0] for i in range(num)], dim=0), torch.stack([train_dataset[i][1] for i in range(num)], dim=0)]\n",
    "    #Here i is the batch and what is 0? dim=0 is the dimension of the output tensor.\n",
    "    #What are IDs???\n",
    "\n",
    "def get_validation_images(num):\n",
    "    return [torch.stack([val_dataset[i][0] for i in range(num)], dim=0), torch.stack([val_dataset[i][1] for i in range(num)], dim=0)]\n",
    "\n",
    "def get_test_images(num):\n",
    "    return [torch.stack([test_dataset[i][0] for i in range(num)], dim=0), torch.stack([test_dataset[i][1] for i in range(num)], dim=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "966c82c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Callback functions\n",
    "\n",
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"saved_models/autoencoder\")\n",
    "\n",
    "class ImageCallback(lightning.pytorch.callbacks.Callback):\n",
    "    def __init__(self, input_imgs, every_n_epochs=1, data_type=\"train\"):\n",
    "        super().__init__()# It's often used to call the parent class's __init__ method to ensure that the parent class is properly initialized when creating an instance of the derived class.\n",
    "        self.input_imgs = input_imgs  # Images to reconstruct during training\n",
    "        # Only save uthose images every N epochs (otherwise tensorboard gets quite large)\n",
    "        self.every_n_epochs = every_n_epochs\n",
    "        self.data_type = data_type\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "            # Reconstruct images\n",
    "            input_imgs = self.input_imgs[0].to(pl_module.device)\n",
    "            input_ids = self.input_imgs[1].to(pl_module.device)\n",
    "            with torch.no_grad():\n",
    "                pl_module.eval()\n",
    "                reconst_imgs = pl_module(input_imgs, input_ids)\n",
    "                pl_module.train()\n",
    "            # Plot and add to tensorboard (only makes sense for images)\n",
    "            #imgs = torch.stack([input_imgs, reconst_imgs], dim=1).flatten(0, 1)\n",
    "            #grid = torchvision.utils.make_grid(imgs, nrow=2, normalize=True, value_range=(-1, 1))\n",
    "            #trainer.logger.experiment.add_image(\"Reconstruction Images %s\" % (self.data_type), grid, global_step=trainer.global_step)\n",
    "\n",
    "            ncols = math.ceil(math.sqrt(input_imgs.shape[0]))\n",
    "            fig, axes = plt.subplots(ncols, ncols, figsize=(10, 10))\n",
    "            for i, ax in enumerate(axes.flat):\n",
    "                if i >= len(input_imgs):\n",
    "                    break\n",
    "                ax.plot(input_imgs[i][0][0].cpu().numpy(), \"-\", color=\"black\")\n",
    "                ax.plot(reconst_imgs[i][0][0].cpu().numpy(), \"--\", color=\"red\")\n",
    "            trainer.logger.experiment.add_figure(\"Reconstructions Plots %s\" % (self.data_type), fig, global_step=trainer.global_step)\n",
    "            \n",
    "\n",
    "class LatentSpaceCallback(lightning.pytorch.callbacks.Callback):\n",
    "    def __init__(self, input_imgs, every_n_epochs=1):\n",
    "        super().__init__()\n",
    "        self.input_imgs = input_imgs  # Images to reconstruct during training\n",
    "        # Only save uthose images every N epochs (otherwise tensorboard gets quite large)\n",
    "        self.every_n_epochs = every_n_epochs\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "            # Reconstruct images\n",
    "            input_imgs = self.input_imgs[0].to(pl_module.device)\n",
    "            input_ids = self.input_imgs[1].to(pl_module.device)\n",
    "            with torch.no_grad():\n",
    "                pl_module.eval()\n",
    "                latent_output = pl_module.encoder(input_imgs, input_ids)\n",
    "                pl_module.train()\n",
    "\n",
    "            # Move latent_output to CPU\n",
    "            latent_output = latent_output.cpu()\n",
    "            try:\n",
    "                histo = torch.histogramdd(latent_output, bins=1_000, range=None)\n",
    "            except RuntimeError:\n",
    "                print(\"RuntimeError\")\n",
    "                return\n",
    "            \n",
    "            # Plot and add to tensorboard as an image\n",
    "            #grid = torchvision.utils.make_grid(histo.unsqueeze(0).unsqueeze(0), normalize=True)\n",
    "            #trainer.logger.experiment.add_image(\"Latent space representation\", grid, global_step=trainer.global_step)\n",
    "\n",
    "            #Plot as a figure (usually better)\n",
    "            fig, axes = plt.subplots(1, 1, figsize=(10, 10))\n",
    "            axes.imshow(histo[0].detach().numpy(), \n",
    "                        extent=[histo[1][0][0], \n",
    "                                histo[1][0][-1],\n",
    "                                histo[1][1][0], \n",
    "                                histo[1][1][-1]], \n",
    "                        aspect='auto', \n",
    "                        origin='lower', \n",
    "                        norm=matplotlib.colors.LogNorm())\n",
    "            trainer.logger.experiment.add_figure(\"Latent space representation\", fig, global_step=trainer.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e63f90d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training function definition\n",
    "\n",
    "def train(latent_dim, base_channel_size, retrain=False):\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "\n",
    "    netName = \"model_ld%i_bc%i_do%i\" % (latent_dim, base_channel_size, derivative_order)\n",
    "    \n",
    "    callbacks=[\n",
    "        #lightning.pytorch.callbacks.ModelCheckpoint(save_weights_only=True),\n",
    "        ImageCallback(get_train_images(36), every_n_epochs=1, data_type=\"train\"),\n",
    "        ImageCallback(get_validation_images(36), every_n_epochs=1, data_type=\"val\"),\n",
    "        lightning.pytorch.callbacks.LearningRateMonitor(\"epoch\"),\n",
    "        lightning.pytorch.callbacks.ModelCheckpoint(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_top_k=1,  # Save only the best checkpoint based on validation loss\n",
    "            save_last=True,  # Save the latest checkpoint\n",
    "            filename='{epoch}-{val_loss:.2f}',  # Customize the filename with epoch and validation loss\n",
    "            verbose=False,\n",
    "        ),\n",
    "        lightning.pytorch.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            patience=EarlyStoppingNr,\n",
    "            verbose=False,\n",
    "            min_delta=0.00,\n",
    "        ),\n",
    "        # Other callbacks...\n",
    "    ]\n",
    "\n",
    "    if latent_dim == 2:\n",
    "        callbacks.append(LatentSpaceCallback(get_train_images(100_000), every_n_epochs=1))\n",
    "\n",
    "    trainer = lightning.Trainer(\n",
    "        default_root_dir=os.path.join(CHECKPOINT_PATH, \"model_%i_%i\" % (latent_dim, base_channel_size)),\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        callbacks=callbacks,\n",
    "        max_epochs=max_epochs,\n",
    "        logger=lightning.pytorch.loggers.tensorboard.TensorBoardLogger(\"saved_models/autoencoder\", \n",
    "                                                                       name=netName),\n",
    "    )\n",
    "    trainer.logger._log_graph = True  # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    #ckpt_files = glob.glob('saved_models/autoencoder/%s/version_0/checkpoints/epoch*.ckpt' % (netName))\n",
    "    ckpt_files = glob.glob('saved_models/autoencoder/%s/version_%i/checkpoints/epoch*.ckpt' % (netName, version_nr))\n",
    "\n",
    "    if ckpt_files and not retrain:\n",
    "        print(\"Found pretrained model, called %s\" % ckpt_files[0])\n",
    "        if variational:\n",
    "            model = VarAutoencoder.load_from_checkpoint(ckpt_files[0])\n",
    "        else:\n",
    "            model = Autoencoder.load_from_checkpoint(ckpt_files[0])\n",
    "        #model.to(\"cuda\")\n",
    "        model.to(device)\n",
    "    else:\n",
    "        if convolutional:\n",
    "            decoder_class = ConvDecoder\n",
    "            encoder_class = ConvEncoder\n",
    "        else:\n",
    "            decoder_class = FcDecoder\n",
    "            encoder_class = FcEncoder\n",
    "\n",
    "        if variational:\n",
    "            model_class = VarAutoencoder\n",
    "        else:\n",
    "            model_class = Autoencoder\n",
    "            \n",
    "        ids_nr = get_test_images(1)[1].shape[1] - 1 \n",
    "        model = model_class(base_channel_size=base_channel_size, \n",
    "                            latent_dim=latent_dim, \n",
    "                            width=derivative_order, \n",
    "                            height=samples_number,\n",
    "                            decoder_class=decoder_class, \n",
    "                            encoder_class=encoder_class,\n",
    "                            dropout_p=dropout_p,\n",
    "                            act_fn=act_fn,\n",
    "                            ids_nr=ids_nr,\n",
    "                            learning_rate=learning_rate)\n",
    "                        \n",
    "        print(\"testing on a single batch...\")\n",
    "        print(model.encoder.forward(get_train_images(100)[0], get_train_images(100)[1]).shape)\n",
    "        print(\"inished testing on a single batch...\")\n",
    "\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        print(\"Finished training, saving model...\")\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, dataloaders=val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "    result = {\"test\": test_result, \"val\": val_result}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "760e82d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pretrained model, called saved_models/autoencoder/model_ld2_bc4_do1/version_0/checkpoints/epoch=85-val_loss=0.00.ckpt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m latent_dim \u001b[38;5;129;01min\u001b[39;00m latent_dim_list:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m nlayers \u001b[38;5;129;01min\u001b[39;00m nLayers_list:\n\u001b[0;32m----> 6\u001b[0m         model_ld, result_ld \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m         model_list\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatent_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: latent_dim, \n\u001b[1;32m      8\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlayers\u001b[39m\u001b[38;5;124m\"\u001b[39m: nlayers,\n\u001b[1;32m      9\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_ld, \n\u001b[1;32m     10\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m: result_ld, \n\u001b[1;32m     11\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m\"\u001b[39m: colors\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)})\n",
      "Cell \u001b[0;32mIn[9], line 57\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(latent_dim, base_channel_size, retrain)\u001b[0m\n\u001b[1;32m     55\u001b[0m         model \u001b[38;5;241m=\u001b[39m Autoencoder\u001b[38;5;241m.\u001b[39mload_from_checkpoint(ckpt_files[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m#model.to(\"cuda\")\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     model\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convolutional:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "#Training loop\n",
    "colors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\", \"k\", \"w\", \"orange\", \"purple\", \"brown\", \"pink\", \"olive\", \"cyan\"]\n",
    "model_list= []\n",
    "for latent_dim in latent_dim_list:\n",
    "    for nlayers in nLayers_list:\n",
    "        model_ld, result_ld = train(latent_dim, nlayers, retrain=retrain)\n",
    "        model_list.append({\"latent_dim\": latent_dim, \n",
    "                           \"nlayers\": nlayers,\n",
    "                           \"model\": model_ld, \n",
    "                           \"result\": result_ld, \n",
    "                           \"color\": colors.pop(0)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a2ceb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Compare loss as a function of latent dimensionality\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m latent_dims \u001b[38;5;241m=\u001b[39m [m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatent_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel_list\u001b[49m]\n\u001b[1;32m      4\u001b[0m nlayers \u001b[38;5;241m=\u001b[39m [m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlayers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m model_list]\n\u001b[1;32m      5\u001b[0m val_scores \u001b[38;5;241m=\u001b[39m [m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m model_list]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_list' is not defined"
     ]
    }
   ],
   "source": [
    " #Compare loss as a function of latent dimensionality\n",
    "\n",
    "latent_dims = [m[\"latent_dim\"] for m in model_list]\n",
    "nlayers = [m[\"nlayers\"] for m in model_list]\n",
    "val_scores = [m[\"result\"][\"val\"][0][\"test_loss\"] for m in model_list]\n",
    "colors = [m[\"color\"] for m in model_list]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "axes[0].scatter(latent_dims, val_scores, color=colors, marker=\"*\", s=100)\n",
    "axes[0].set_xlabel(\"Latent Dimensionality\")\n",
    "axes[0].set_ylabel(\"Validation Loss\")\n",
    "\n",
    "axes[1].scatter(nlayers, val_scores, color=colors, marker=\"*\", s=100)\n",
    "axes[1].set_xlabel(\"Number of Layers\")\n",
    "axes[1].set_ylabel(\"Validation Loss\")\n",
    "\n",
    "# Scatter plot with latent_dim on x-axis, nlayers on y-axis, and colormap on z-axis (val_scores)\n",
    "scatter = axes[2].scatter(latent_dims, nlayers, c=val_scores, cmap='viridis', marker=\"s\", s=600)\n",
    "axes[2].set_xlabel(\"Latent Dimensionality\")\n",
    "axes[2].set_ylabel(\"Number of Layers\")\n",
    "axes[2].set_title(\"Validation Loss\")\n",
    "\n",
    "# Add colorbar\n",
    "cbar = fig.colorbar(scatter, ax=axes[2])\n",
    "cbar.set_label(\"Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18310b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define plot functions\n",
    "\n",
    "def plot(inTensor, axes, color, dim, label, linestyle=\"--\", skip=0):\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i >= len(inTensor)+skip:\n",
    "            break\n",
    "        if i < skip:\n",
    "            continue    \n",
    "        ax.plot(inTensor[i-skip][0][dim].numpy(), linestyle, color=color, label=label)\n",
    "        ax.legend()\n",
    "\n",
    "def reconstruct_signals(input_signal, model_l, axes, dim):\n",
    "    # Reconstruct images\n",
    "    model = model_l[\"model\"].eval()\n",
    "    color = model_l[\"color\"]\n",
    "    label = \"ld %s nl %s\" % (model_l[\"latent_dim\"], model_l[\"nlayers\"])\n",
    "    with torch.no_grad():\n",
    "        reconst_imgs = model(input_signal[0].to(model.device), input_signal[1].to(model.device))    \n",
    "    reconst_imgs = reconst_imgs.cpu()\n",
    "    plot(reconst_imgs, axes, color, dim, label)\n",
    "\n",
    "def generate_signals(input_latents, model_l, axes, dim, skip=0, color=\"b\"):\n",
    "    # Reconstruct images\n",
    "    model = model_l[\"model\"].eval()\n",
    "    label = \"e=%.1f, x=%.1f, y=%.1f\" % (input_latents[1][0][0].numpy(), input_latents[0][0][0].numpy(), input_latents[0][0][1].numpy())\n",
    "    with torch.no_grad():\n",
    "        reconst_imgs = model.decoder(input_latents[0].to(model.device), input_latents[1].to(model.device))    \n",
    "    reconst_imgs = reconst_imgs.cpu()\n",
    "    plot(reconst_imgs, axes, color, dim, label, skip=skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bc690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare the reconstructions with different model hyper-parameters\n",
    "ncols = 4\n",
    "nrows = 16\n",
    "dim = 0\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(15, 40))\n",
    "input_imgs = get_train_images(ncols*nrows)\n",
    "#input_imgs = get_validation_images(ncols*nrows)\n",
    "\n",
    "for m in model_list:\n",
    "    reconstruct_signals(input_imgs, m, axes, dim)\n",
    "plot(input_imgs[0], axes, \"black\", dim=dim, label=\"input\", linestyle=\"-\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af705ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the latent space\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import ipywidgets as widgets\n",
    "    \n",
    "# Create the slider\n",
    "ene_slider = widgets.FloatSlider(value=0.5, min=0, max=1, step=0.01, description='Energy:')\n",
    "nch = get_train_images(1)[1].shape[1] - 1\n",
    "ch_dropdown = widgets.Dropdown(\n",
    "    options=[(f'Channel {i}', i) for i in range(-1,nch+1)],\n",
    "    value=0,\n",
    "    description='Channel:',\n",
    ")\n",
    "# Display the slider\n",
    "display(ch_dropdown)\n",
    "display(ene_slider)\n",
    "\n",
    "idx_test = 0\n",
    "\n",
    "def get_histo(idx_ch):\n",
    "    with torch.no_grad():\n",
    "        model_list[idx_test][\"model\"].eval()\n",
    "        dataset = get_train_images(500_000)\n",
    "        # Filter the dataset\n",
    "        if idx_ch >= 0:\n",
    "            dataset = (dataset[0][dataset[1][:, idx_ch+1] == 1], \n",
    "                       dataset[1][dataset[1][:, idx_ch+1] == 1])\n",
    "        avg_id = dataset[1].mean(dim=0)\n",
    "\n",
    "        \n",
    "\n",
    "        histoOut = torch.histogramdd(   model_list[idx_test][\"model\"].encoder(dataset[0].to(model_list[idx_test][\"model\"].device), \n",
    "                                                                                dataset[1].to(model_list[idx_test][\"model\"].device)),\n",
    "                                        bins=5_000,\n",
    "                                        range = None,\n",
    "                                        )\n",
    "        histoOut[0].detach().numpy()\n",
    "\n",
    "        return [histoOut, avg_id]\n",
    "\n",
    "\n",
    "#Create the 2 canvases\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
    "\n",
    "                \n",
    "def on_ch_dropdown_change(change):\n",
    "    value = change['new']\n",
    "    histo, avg_id = get_histo(value)\n",
    "\n",
    "    colors=[\"b\", \"g\", \"r\", \"c\", \"m\", \"y\", \"k\", \"w\", \"orange\", \"purple\", \"brown\", \"pink\", \"olive\", \"cyan\"]\n",
    "\n",
    "    axes[0].imshow( histo[0], \n",
    "                    extent=[histo[1][0][0], \n",
    "                            histo[1][0][-1],\n",
    "                            histo[1][1][0], \n",
    "                            histo[1][1][-1]], \n",
    "                    aspect='auto', \n",
    "                    origin='lower', \n",
    "                    norm=matplotlib.colors.LogNorm())\n",
    "                    \n",
    "    def on_histo_double_click(event):\n",
    "        if event.dblclick:\n",
    "            x = event.xdata\n",
    "            y = event.ydata\n",
    "            if x is not None and y is not None:\n",
    "\n",
    "                #Get coordinates\n",
    "                last_click = [x, y] \n",
    "                print(\"[%.2f, %.2f]\" % (x, y))\n",
    "\n",
    "                color=colors.pop(0)\n",
    "                #Annote in the plot\n",
    "                axes[0].annotate(\"[%.2f, %.2f]\" % (x, y), (x, y), color=color, bbox=dict(facecolor='white', edgecolor='white', alpha=0.5))\n",
    "                axes[0].plot(x, y, 'o', color=color)\n",
    "\n",
    "                #Plot the signal\n",
    "                def on_ene_slider_change(change):\n",
    "                    value = change['new']\n",
    "                    double_click_tensor = torch.FloatTensor([last_click])\n",
    "                    id_tensor = avg_id.unsqueeze(0).repeat(double_click_tensor.size(0), 1)\n",
    "                    id_tensor[:, 0] = value\n",
    "                    generate_signals([double_click_tensor, id_tensor], model_list[idx_test], axes, dim=0, skip=1, color=color)\n",
    "\n",
    "                ene_slider.observe(on_ene_slider_change, names='value')\n",
    "\n",
    "    fig.canvas.mpl_connect('button_press_event', on_histo_double_click)\n",
    "\n",
    "ch_dropdown.observe(on_ch_dropdown_change, names='value')            \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b81e4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding similar signals in the latent space and then checking the signals\n",
    "\n",
    "def embed_imgs(model, data_loader):\n",
    "    # Encode all images in the data_laoder using model, and return both images and encodings\n",
    "    img_list, embed_list = [], []\n",
    "    model.eval()\n",
    "    for imgs, ids in data_loader:\n",
    "        with torch.no_grad():\n",
    "            z = model.encoder(imgs.to(model.device), ids.to(model.device))\n",
    "        img_list.append(imgs)\n",
    "        embed_list.append(z)\n",
    "    return (torch.cat(img_list, dim=0), torch.cat(embed_list, dim=0))\n",
    "\n",
    "\n",
    "train_img_embeds = embed_imgs(model_list[idx_test][\"model\"], train_loader)\n",
    "test_img_embeds = embed_imgs(model_list[idx_test][\"model\"], test_loader)\n",
    "\n",
    "def find_similar_images(query_img, query_z, key_embeds, ax, K=8, dim=0):\n",
    "    # Find closest K images. We use the euclidean distance here but other like cosine distance can also be used.\n",
    "    dist = torch.cdist(query_z[None, :], key_embeds[1], p=2)\n",
    "    dist = dist.squeeze(dim=0)\n",
    "    dist, indices = torch.sort(dist)\n",
    "    # Plot K closest images\n",
    "    imgs_to_display = torch.cat([query_img[None], key_embeds[0][indices[:K]]], dim=0)\n",
    "    #grid = torchvision.utils.make_grid(imgs_to_display, nrow=K + 1, normalize=True, value_range=(-1, 1))\n",
    "    #grid = grid.permute(1, 2, 0)\n",
    "    #plt.figure(figsize=(12, 3))\n",
    "    #plt.imshow(grid)\n",
    "    #plt.axis(\"off\")\n",
    "    #plt.show()\n",
    "    colors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\", \"k\", \"w\", \"orange\", \"purple\", \"brown\", \"pink\"]\n",
    "    ax.plot(query_img[0][dim].numpy(), color=\"k\", label=\"query\")\n",
    "    for i in range(imgs_to_display.size()[0]):\n",
    "        ax.plot(imgs_to_display[i][0][dim].numpy(), \"--\", color=colors.pop(0), label=\"latent_dim %i\" % latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc1092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the closest images for the first N test images as example\n",
    "ncols = 6\n",
    "nrows = 6\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    find_similar_images(test_img_embeds[0][i], test_img_embeds[1][i], key_embeds=train_img_embeds, K=6, ax=ax, dim=0)\n",
    "\n",
    "#This does not make sense, i probabily did something wrong"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
